[Исчтоник](https://en.wikipedia.org/wiki/Pointwise_mutual_information)
#count-based-Method, #embeddings, #NLP

## Count-based methods
Эти методы решают задачу прямоленейно. 
>Count-based methods вносят информацию о контексте основываясь на global corpus statistics, то есть на глобальной статистике нашего корпуса текста.

![[Pasted image 20220515000420.png]]
Общий алгоритм этих методов:
1. Конструирование word-contex матрицы;
2. Уменьшаем её размерности.
	* для уменьшение занимаемой памяти;
	* для избавления от неинформативных контекстов, ведь слова появляются лишь в небольшом количестве контекстов. 

>dot product word/contex нормализированных векторов даёт близость/похожесть контекста к слову (или слова к контексту)

Для определния конкретного метода, требуется понять, что будет:
1. Possible contexts. Что вообще значит контекст? 
2. The notion of association - формула для расчёта элементов нашей матрицы.

## Positive Pointwise Mutual Information (PPMI)
Контекст в этом методе определён также, но мера взаимосвязи слова и контекста более сложная: 
Суть PMI в получении числового значения правдоподобности того, что эти слова встретились неслучайно, при этом он учитывает вероятность того, что эти слова могут быть в одном контексте случайно.
**Например**. New York это не просто новый йорк - это город. Это словосочетание несёт в себе смысл и вероятность появление этих слов вместе высока. В то время как old garden в тексте про сады не несёт в себе той смысловой нагрузке, что New York это просто старый сад и эти слова скорее встретились случайно, чем это для них прям характерно garden может быть и nice, glow и blooming.
$$PMI(w, c)=\log{\frac{P(w, c)}{P(w)P(c)}}=\log{\frac{P(w|c)}{P(w)}}=\log{\frac{P(c|w)}{P(c)}}=\log{\frac{N(w,c)*n}{N(w)N(c)}}$$
>Заметка. На [wiki](https://en.wikipedia.org/wiki/Pointwise_mutual_information) используется $\log_2$

$PMI$ значения.
- $P(w, c)=P(w)P(c)$ - слова встречаются вместе случайно - они независимы ($\log=0$);
- $P(w, c)>P(w)P(c)$ - слова встречаются вместе чаще чем случайно ($\log>0$);
- $P(w, c)<P(w)P(c)$ - слова встречаются вместе реже чем даже случайно ($\log<0$).

$PPMI(w, c)=\max(0, PMI(w, c))$ и есть positive он отрубает отрицательные значения. Мотивация у него следующая:
- $PPMI$ появился из наблюдения, что отрицательные значения $PMI$ ненадёжны (слова встречаются реже чем случайно -> корпус не полный), только если наш корпус текста не огромный;
- также он позволяет избежать $P(w, c)=0$ (то есть слова не встречаются вместе). Этот случай плох тем, что $\log_2{0}=-\infty$

>На заметку. Было показано, что некоторые нейро-методы (например Word2Vec) неявно аппроксимируют факторизацию ( #непонял ) (сдвинутой) PMI матрицы. 

>[!Summarize]
>**Контекст**:
>Окружающие слова в окне размера L
>
> **Элемент матрицы:**
> $$PPMI(w, c)=\max(0, PMI(w, c))$$
> $$PMI(w, c)=\log{\frac{P(w, c)}{P(w)P(c)}}=\log{\frac{N(w,c)*n}{N(w)N(c)}}$$
