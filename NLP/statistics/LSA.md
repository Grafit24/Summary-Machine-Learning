---
tags:
  - count-based-method
  - embeddings
  - NLP
  - tf-idf
aliases:
  - tf-idf
  - Latent Semantic Analysis
---
[Источник]()
#count-based-method #embeddings #NLP

## Count-based methods
Эти методы решают задачу прямоленейно. 
>Count-based methods вносят информацию о контексте основываясь на global corpus statistics, то есть на глобальной статистике нашего корпуса текста.

![[Pasted image 20220515000420.png]]
Общий алгоритм этих методов:
1. Конструирование word-contex матрицы;
2. Уменьшаем её размерности.
	* для уменьшение занимаемой памяти;
	* для избавления от неинформативных контекстов, ведь слова появляются лишь в небольшом количестве контекстов. 

>dot product word/contex нормализированных векторов даёт близость/похожесть контекста к слову (или слова к контексту)

Для определния конкретного метода, требуется понять, что будет:
1. Possible contexts. Что вообще значит контекст? 
2. The notion of association - формула для расчёта элементов нашей матрицы.

## Latent Semantic Analysis (LSA)
![[Pasted image 20230913174838.png]]

### TF-IDF
TF-IDF это аббревиатура от "Term Frequency - Inverse Document Frequency". Это статистическая мера, используемая для оценки важности слова в документе, который является частью коллекции или корпуса.

- TF (Term Frequency) означает частотность слова — как часто слово встречается в тексте. Интуитивно понятно, что чем чаще слово встречается в тексте, тем оно важнее для этого текста.
    
- IDF (Inverse Document Frequency) это обратная частота документа. Она уменьшает вес слов, которые часто встречаются во всех документах (например, предлоги), и увеличивает вес редких или специфических слов.
    

Таким образом, TF-IDF - это способ оценить, насколько слово важно для конкретного документа, на фоне всех остальных документов. Если слово встречается часто в конкретном документе, но редко встречается в других документах, то у этого слова будет высокое значение TF-IDF.

Это метод анализа коллекции документов, как следует в качестве контекста берётся весь документ, а не просто окно. То есть суть этого метода состоит в анализе связей между документами и терминами (словами) внутри них содержащихся.

### LSA

> LSA предполагает, что слова близкие по значению встречаются в похожих текстах.

Так cosine similiarity между векторами документов, состоящими из числовых характерестик каждого слова, может использоваться для определения схожести документов.

LSA часто отсылыет к более общему подходу применения SVD апроксимации к term-document матрицы, элементы которой могут вычислятся по разному (e.g., simple co-occurrence, tf-idf, or some other weighting). 

Пример такой матрицы можно увидеть на видео ниже. На нём показан процесс определния тем документов. Столбец отсылает к документу, а строка к слову, чем темнее цвет элемента тем больше его вес. Сначала сортируются документы по близости, после чего сортируются слова.![[Topic_model_scheme.webm.480p.vp9.webm]]