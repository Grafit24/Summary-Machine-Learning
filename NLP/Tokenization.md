---
tags:
  - tokenization
  - NLP
---
[Источник](https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/)
### Что такое токенизация?
Токенизация - это способ разбиения фрагмента текста на более мелкие блоки - **токены**. 

То есть токенизацию в основе своей можно разделить на три больших класса: 
- символьная: s-m-a-r-t-e-r;
- n-грамная (аля токенизация подслов): smart-er;
- токенизация слов: smarter. 

### Word Tokenization
Наиболее популярный способ разбиение текста. Делит текста на слова по некоторому разделителю.

**Проблемы**: 
- Новые слова (которых не было в изначальном корпусе) неизвестны и их приходится обрабатывать, как UNK, то есть как неизвестные. Из-за токенизация слова, как UNK мы теряем его структуру, что могло бы быть полезно для модели.
- Большой словарь получается. Ну можно представить просто создавать токен для каждого уникального слова :) 

### Character Tokenization
Разбиение текста на токены символов. Получается маленький словарь (256-1024 токена).

**Проблемы**: Количество входных и выходных токенов (для случая генерации) возрастает невероятно стремительно из-за чего RNN по своей сути может ухватить лишь смысл одного или пары слов, например.

### Subword Tokenization
Или токенизация на n-грамы (например lower -> low-er). Это что-то среднее между word и charachter токенизацией. 

#### [[BPE|Byte Pair Encoding]] (BPE)
Один из наиболее популярных способов subword tokenization и токенизация для трансформеров. BPE разбивает слова на сегменты таким образом, чтобы соединить символы, которые наиболее часто встречаются вместе и производит это итеративно начиная с изначального разбиения на отдельные символы (изначально разбивается текст на слова).

**Плюсы**:
- BPE разбирается с неизвестными словами, разбивая их на подслова и сохраняя какую-некакую информацию о структуре слова.
- Длина токенизированного текста при этом заметно меньше charachter tokenization.