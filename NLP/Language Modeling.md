#NLP #language-modeling 
[NLP Course](https://lena-voita.github.io/nlp_course/language_modeling.html)
### Что такое Language Modeling?
Это моделирование языка, то есть предсказания лингвистических единиц (слов, предложений, символов) в тексте, основываясь на контексте. То есть это авторегрессионная модель. 

### General framework
Пусть $(y_1, y_2, ..., y_n)$ последовательность токенов, а $P(y_1, y_2, ..., y_n)$ это вероятность встретить эту последовательность токенов. Используя product rule of probability получаем:
$$P(y_1, y_2, ..., y_n)=P(y_1)P(y_2|y_1)...P(y_n|y_1, ..., y_{n-1})=\prod_{t=1}^nP(y_t|y_{<t})$$
Это и есть General framework (Left-to-Right LM Framework). Модели отличаются только способом вычисления вероятностей.

**Generation using LM.**
Способы:
- Сэмплируем из распределения вероятностей.
- Greedt decoding. Берётся наиболее вероятное слово из распределения.

### [[N-gram]]
Один из способов построение LM. Вычисляет вероятности используя [[Word Embeddings|global corpus statistics]], например просто считая кол-во вхождений в корпус текста.
**Марковское св-во**. Вероятность слова зависит только от фиксированного кол-ва предыдущих слов.
То есть для n-gram model:
$$P(y_t|y_1, ..., y_{t-1})=P(y_t|y_{t-n+1}, ..., y_{t-1})=\frac{N(y_1, y_2, ..., y_t)}{N(y_1, y_2, ..., y_{t-1})}$$
- $n=3$ - trigram model $P(y_t|y_{t-n+1}, ..., y_{t-1})=P(y_t|y_{t-2}, t_{t-1})$
- $n=2$ - bigram model $P(y_t|y_{t-n+1}, ..., y_{t-1})=P(y_t|t_{t-1})$
- $n=1$ - unigram model $P(y_t)$

### Neural Language Models
Использование нейросетей для подсчёта верояностей -> то есть задача [[Text Classification|классификация]] на $|V|$ классов. Обучается с помощью [[Cross Entropy]]. Распределение вероятностей получаем с помощью [[Softmax]].

![[Pasted image 20230311200900.png]]

![[nn_lm_prob_idea.mp4]]

Models:
- [[RNN]]
- [[LSTM]]
- [[GRU]]

### Generation strategies
Нам нужно, чтобы сгенерированный текст был:
- **coherent** (осмысленным)
- **diverse** (разнообразным)

**Sampling strategies**:
- **Standart sampling**. Сэмплим слчайном из распределения вероятностей следвующего токена, которые мы получили с помощью LM (NN или [[N-gram]])
- **Sampling with Temperature**. Меняем [[Softmax]] temperature (делим логиты на $\tau$).
	- Применение температуры![[Pasted image 20230311210112.png]]
	- Влияние $\tau$ на diversity и coherence![[Pasted image 20230311210632.png]]
	- [Визуализация](https://lena-voita.github.io/nlp_course/language_modeling.html#evaluation:~:text=Sampling%20with%20temperature)
- **Top-K sampling**. Сэмплируем только топ $k$ наиболее вероятных токенов.
	- Не всегда хорошо![[Pasted image 20230311212358.png]]
		- Охватывает малую часть от всей probability mass
		- Может содержать маловероятные токены
- **Top-p (aka Nucleus) sampling**. Сэмплируем $p\%$ от всей probability mass.
	- ![[Pasted image 20230311212801.png]]

### Evaluating Language Models
**Идея**. Если модель "хорошая", то показав ей новый для неё "реальный" текст - он будет для неё ожидаемым.

![[Pasted image 20230317172154.png]]

Формализация "ожидаемого для модели" происходит через [[Perplexity]]. Чем она меньше тем лучше модель. 
- Лучшая [[Perplexity]] = 1
- Худшая [[Perplexity]] = |V|

### Practical tips
1. **Weight Tying**. Использование одной и той же матрицы эмбеденгов на входе и на выходе, ведь что одна что другая это матрица эмбедингов слов в словаре. Также такое двойное использование матрицы эмбеденгов позволяет сократить расходы памяти.
![[Pasted image 20230317210116.png]]

2. **Analysis and Interpretability**. Входе моделирования языка, некоторые нейроны модели могут быть вполне интерпретируемы, например, Sentiment Neuron, который умеет определять настроение текста (например негативный или позитивный отзыв). Такой нейрон можно использовать, как для классификации настроения текста, так и для контроля генерации текста.
![[Pasted image 20230317213623.png]]
3. **Contrastive Evaluation: Test Specific Phenomena**. Чтобы узнать насколько хорошо модель справляется с какой-нибудь специфической задачей, то есть выучила ли модель что-то полезное из текста. Можно сравнить её вероятности на двух одинаковых сэмплах, только с тем отличием, что в них изменена интересующая нас деталь. 
   Так, например, нас интересует умеет ли наша модель определять множественное число и правильно ставить is/are:
![[Pasted image 20230317214115.png]]
![[Pasted image 20230317214038.png]]