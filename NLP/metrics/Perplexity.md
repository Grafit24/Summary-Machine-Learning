---
tags:
  - NLP
  - metric
  - language-modeling
---
[Habr](https://habr.com/ru/company/wunderfund/blog/580230/) [NLP Course](https://lena-voita.github.io/nlp_course/language_modeling.html#evaluation)
## Perplexity
Перплексия - это способ оценки качества языковых моделей, который берёт в свою основу интуицию, что если нашу модель не удивляет (вероятности правильных токенов высокие) новый (настоящий) текст, то она является хорошей моделью. Её можно определить через [[Cross Entropy]].

$$\text{Perplexity}(y_{1:M})=\frac{1}{\sqrt[M]{P(y_1, y_2, ..., y_M)}}=2^{-\frac{1}{M}\sum^M_{t=1} \log_2{p(y_t|y_{<t})}}$$

![[Pasted image 20230321184302.png]]

Чем меньше перплексия тем лучше языковая модель:
- **Лучший случай**. Если перплексия равна 1, то тогда языковая модель с идеальной точностью может воссоздать текст (что в реально жизни не бывает). Другими словами она ставит вероятность 1 для всех верных токенов.
- **Худший случай**. Если перплексия равна $|V|$, то тогда языковая модель равновероятно предсказывает каждый токен в словаре, то есть вероятность каждого токена будет $\frac{1}{|V|}$. 
$$Perplexity(y_{1:M})=2^{-\frac{1}{M}L(y_{1:M})} =
            2^{-\frac{1}{M}\sum\limits_{t=1}^M\log_2 p(y_t|y_{1:t-1})}=
            2^{-\frac{1}{M}\cdot M \cdot \log_2\frac{1}{|V|}}=2^{\log_2 |V|} =|V|.$$

## Интерпретация
Преплексию можно интерпретировать, как взвешенное (на вероятности) кол-во ветвлений (кол-во возможных исходов), то есть из скольки слов в среднем модели приходится выбирать следующий токен. 

Пример, если преплексия равна $N$ это означает, что в среднем когда модели приходится предсказазывать следующий токен, то он её вводит в такую же "расстерянность", как если бы ей пришлось выбирать одно из $N$ равновероятных исходов.  

> [[Cross Entropy]] указывает на среднее количество бит, необходимых для кодирования одного слова, а перплексия — это количество слов, которое может быть закодировано с помощью этих битов.