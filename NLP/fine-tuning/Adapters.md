---
tags:
  - PEFT
  - fine-tuning
  - transformer
  - NLP
---
Вся модель остаётся freezed файнются только небольшое кол-во специальных модулей Adapters за счёт уменьшенного hidden-size (r << d) обновляется лишь крошечный объём параметров относительно целой модели ~1%. 

![[Pasted image 20240118164411.png]]