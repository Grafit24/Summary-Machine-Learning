#CV #NLP #attention 
[NLP Course](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
## "Look at each other"
Self-Attention это часть модели, позволяющая каждому токену понять себя лучше в контексте, посмотрев на другие токены, и обновляя собсвтенное представление.  

Разница между [[Attention]] и [[Self-Attention]] заключается в том, что self-attention оперерует представлениями одной природы (например: состояния энкодера)

![[Pasted image 20230412192908.png]]

![[encoder_self_attention.mp4]]

## Query, Key and Value
Каждый входной токен получает три репрезентации в соответствии с исполняемым им ролью:
- **Query** - спрашивает информацию.
- **Key** - говорит о наличии информации.
- **Value** - передаёт информацию.

![[Pasted image 20230412193917.png]]

Query используется для запроса информации нужной информации у других токенов, ключ это инфорфмация имеющаяся у токена. Перемножая $\text{query} \times \text{key}$ и применяя софтмакс получаем вес для значения этого токена (вектор value). Взвешенно суммируя эти значения получаем новое представление токена. Это происходит для каждого токена последовательности (параллельно).

![[Pasted image 20230412193922.png|600]]
$d_K$ - это размерность вектора $k$ и $v$. Корень эвристика (было выяснено эмпирически). 