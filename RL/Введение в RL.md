#RL 
[Лекция](https://www.youtube.com/watch?v=2iz5KSSsfeg)
## Обучение с подкреплением (aka RL)
Этот подраздел машинного обучения занимается созданием агентов способных обучаться в динамической среде.

Основная цель максимзировать награду:
![[Pasted image 20230509193929.png|500]]
![[Pasted image 20230509194317.png]]
$\gamma \in (0,1)$ - discount factor нужен чтоб сумма сходилась

Награда может быть **dense** или **sparse**. То есть часто или редко (**sparse** сложнее).

![[Pasted image 20230509193734.png]]

Основополагающим является марковский процесс принятия решений (MDP). Задается с помощью:
- множества состояний
- множества действий
- функции награды
- функции перехода в новое состояние (динамика)

**Марковское свойство** - условное распределение вероятностей будущих состояний и наград зависит только от нынешнего состояния.
![[Pasted image 20230509200208.png]]
![[rl-offline-online.gif]]

## Policy Gradients
В RL мы не можем найти градиент от кумулятивной награды по параметрам т.к. в общем случае среда **не дифференцируемая**. Так, как это сделать?

**Мат. ожидание кумулятивной награды** для данной политики – то, что мы хотим максимизировать:
![[Pasted image 20230509202921.png]]

Если получится найти градиент, то мы сможем использовать стандартные методы из deep learning для обновления параметров политики:
![[Pasted image 20230509202951.png]]

![[Pasted image 20230509202845.png]]

Док-во в лекции. (В идеале добавить)

![[Pasted image 20230509202834.png]]


## Продолжение
**Курсы**:
- [Huggingface Deep RL](https://huggingface.co/learn/deep-rl-course/unit0/introduction)
- Open AI Spinning Up
- CS285

**Учебники:**
- Sutton & Barton: Reinforcement Learning: An Introduction

Ссылки:
- [OpenAI база](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)