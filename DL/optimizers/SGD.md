# Gradient Descent
[Источник](https://ruder.io/optimizing-gradient-descent/)
#optimizer 
## SGD mini-batch
Оптимизация весов модели по градиентам, полученных с loss function относительно весов $θ$, с некоторым learning rate $η$, по её mini-batch'ам, а не конкретным примерам или всей выборки. Для обновление параметров мы усредняем по mini-batch градиенты.
$θ=θ−η⋅∇_θJ(θ;x^{(i:i+n)};y^{(i:i+n)})$

## Vanila GD
Оптимизация по всей выборки. Для обновление параметров мы усредняем градиенты.
$θ=θ−η⋅∇_θJ(θ)$

## SGD
Оптимизация по одному примеру.
$θ=θ−η⋅∇_θJ(θ;x^{(i)};y^{(i)})$


## Проблемы классического подхода
- выбор learning rate;
- отсутствие регулировки learning rate в течение обучения;
- одинаковый learning rate для данных разной частоты;
- попадание в suboptimal minimum.