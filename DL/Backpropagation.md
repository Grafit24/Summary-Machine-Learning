Backpropagation — это метод вычисления градиента, используемый для обновления весов в нейронной сети. 

## Суть метода
Суть можно выразить напрямую из правила дифференцирования сложной функции (chain rule). Пусть $f(x)=g_n(g_{n-1}(...(g_1(x))...))$, тогда $\frac{\partial f}{\partial x}=\frac{\partial f}{\partial g_n}\frac{\partial g_n}{\partial g_{n-1}}...\frac{\partial g_1}{\partial x}$. То есть метод путём обратного прохода сети начиная с $\frac{\partial g_n}{\partial g_{n-1}}$ позволяет использовать вычисленные частные производные предыдущих слоёв для вычисления последующих. 

## Алгоритм
1. Froward pass. Вычисляем выход каждого слоя с входного до последнего.
2. Backward pass. Вычисляем все градиенты.
3. Совершаем шаг [[SGD]] с полученными на предыдущем шаге градиентами.