#NLP #CV
## What is LLM?
–ú–æ–¥–µ–ª–∏ –∏–º–µ—é—â–∏–µ >1b –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ —Ç–∞–∫–∏–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ—Ç —É–≤–µ–ª–µ—á–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –ø–æ–ª—É—á–∞—é—Ç –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞.

LLMs:
- [[GPT 3 üîÑ|GPT 3]]/3.5/4 - [OpenAI](https://openai.com/research/language-models-are-few-shot-learners)
- [[T5]] - [Arxiv](https://arxiv.org/abs/1910.10683)
- BLOOM
- [[BART]] - [Arxiv](https://arxiv.org/abs/1910.13461)

–Ø–≤–ª—è–µ—Ç—Å—è –ª–∏ [[BERT]] LLM - –æ—Ç–≤–µ—Ç –Ω–µ—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –æ—Ç —É–≤–µ–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞. –ó–∞–¥–∞—á–∞ –∑–∞–ø–æ–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–∂–Ω–æ–π.

## Parallelism
–û–ø–µ—Ä–∞—Ü–∏–∏:
- Scatter - –æ—Ç–ø—Ä–∞–≤—å –¥–∞–Ω–Ω—ã–µ 
- Gather - –≤–æ–∑—å–º–∏ –¥–∞–Ω–Ω—ã–µ
- Broadcast - —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –≤—Å–µ –Ω–æ–¥—ã
![[Pasted image 20230417195222.png]]
- AllReduce operation is performing reductions on data (for example, sum, max) across devices and writing the result in the receive buffers of every rank.
![[Pasted image 20230417195111.png]]

**[[Data Parallel]]**:
- –û–¥–Ω–∏ –∏ —Ç–µ –∂–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –≤—Å–µ—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö (–≤ –∫–∞–∂–¥–æ–º –¥–µ–≤–∞–π—Å–µ –ø–æ –º–æ–¥–µ–ª–∏)
- –ü–æ—Å–ª–µ —Å—Ç–µ–ø–∞ –ø—Ä–∏–º–µ–Ω—è–µ–º AllReduce –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∏–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã), —É—Å—Ä–µ–¥–Ω—è—è –∏—Ö
- –¢–µ–º —Å–∞–º—ã–º –º—ã –ø–æ–ª—É—á–∞–µ–º —Å–Ω–æ–≤–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Å—Ç–µ–ø–∞
- [Torch](https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html)

**[[Distributed Data Parallel]]**:
- –ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–µ –Ω–∞ —Ç—Ä–µ–¥–∞—Ö, –∞ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö
- [Torch](https://pytorch.org/docs/stable/distributed.html)

![[Pasted image 20230417201155.png|300]]

**[[Pipelined Model Parallel]]**:
- –î–µ–ª–∏–º –º–æ–¥–µ–ª—å –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ —Å–ª–æ—è–º –∏ —Ö—Ä–∞–Ω–∏–º —ç—Ç–∏ —á–∞—Å—Ç–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏
- –í–æ –≤—Ä–µ–º—è forward –ø–µ—Ä–µ–∫–∏–¥—ã–≤–µ—Ç –≤—ã—Ö–æ–¥—ã —á–∞—Å—Ç–∏ —Å –æ–¥–Ω–æ–π –Ω–∞ –≤—Ö–æ–¥ –¥—Ä—É–≥–æ–π
- –¢–æ–∂–µ —Å–∞–º–æ–µ –≤–æ –≤—Ä–µ–º—è backward
- [Arxiv](https://arxiv.org/abs/2104.04473)

![[Pasted image 20230417201700.png|300]]

**[[Tensor Model Parallel]]**:
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ—ë–≤ –º–µ–∂–¥—É –¥–µ–≤–∞–π—Å–∞–º–∏
- [Arxiv](https://arxiv.org/abs/1909.08053)

![[Pasted image 20230417202909.png]]

![[Pasted image 20230417204000.png|500]]

**[[Sequence Model Parallel]]**:
- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª—è [[Dropout]] –∏ [[Layer Normalization]]
- [Arxiv](https://arxiv.org/abs/2205.05198)

**Gradients/parameters/optimizer [[Offloading]]**:
- –ü—Ä–µ–¥–ª–∞–≥–∞–µ—Ç—Å—è —Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º –¥–µ–≤–∞–π—Å–µ —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö, –∞ –ø—Ä–∏ –∏—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –∏—Ö –ø–æ –±—ã—Å—Ç—Ä–æ–π —à–∏–Ω–µ.
- –¢–∞–∫–∂–µ –∏—Ö –º–æ–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤ RAM –∏–ª–∏ SSD.
- –¢–∞–∫ —Ö—Ä–∞–Ω–µ–Ω–∏–µ Optimizer States (–≤ —Å–ª—É—á–∞–µ [[Adam]] —ç—Ç–æ –ø–µ—Ä–≤—ã–µ –∏ –≤—Ç–æ—Ä—ã–µ –º–æ–º–µ–Ω—Ç—ã –≤ —Ç–æ–º —á–∏—Å–ª–µ) –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Ä–∞—Å—Ö–æ–¥ –ø–∞–º—è—Ç–∏ –≤ 4 —Ä–∞–∑–∞. –¢–æ–∂–µ —Å–∞–º–æ–µ –º–æ–∂–Ω–æ –ø—Ä–æ–¥–µ–ª–∞—Ç—å –∏ —Å gradients –∏ parameters. ![[Pasted image 20230417210443.png]]
- [Arxiv](https://arxiv.org/abs/1910.02054)

**Extreme case of distributed models**:
- –ì–æ–º–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∫–æ–≥–¥–∞ –≤—Å–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã.
	- –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –æ–±—É—á–∞–µ–º –Ω–∞ –Ω–∞—à–µ–º —Å–µ—Ä–≤–µ—Ä–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö tesla v100
- –ì–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å - –≤—Å–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ —Ä–∞–∑–ª–∏—á–Ω—ã.
	- –ù–∞–ø—Ä–∏–º–µ—Ä, –º—ã –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø–æ –º–∏—Ä—É —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.
	- [Hivemind](https://github.com/learning-at-home/hivemind)

## Training Precision
–û–±—ã—á–Ω–æ fp32, —á—Ç–æ –¥–æ–≤–æ–ª—å–Ω–æ –º–Ω–æ–≥–æ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –ø–∞–º—è—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º fp16, –Ω–æ —Ç–∞–∫ –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫–æ—Ç–æ—Ä—ã–º –Ω–µ–æ–±—Ö–æ–¥–∏–º fp32, —Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è

**[[Automatic Mixed Precision]]**:
- –ß–∞—Å—Ç—å –æ–ø–µ—Ä–∞—Ü–∏–π –≤ fp32 —á–∞—Å—Ç—å –≤ fp16
- –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏
- –ë—ã—Å—Ç—Ä–µ–µ –Ω–∞ GPU
- ![[Pasted image 20230417213258.png]]
- [Torch](https://pytorch.org/docs/stable/amp.html)

–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π —Ç–∏–ø float: [bf16/tf16](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/)
![[Pasted image 20230417214102.png|500]]

Quantization. 8 bit.
- LLM.int8 [Arxiv](https://arxiv.org/abs/2208.07339)
	- outliers –∑–Ω–∞—á–µ–Ω–∏—è (—Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ) —ç—Ç–æ –Ω–∞—à–∞ –ø—Ä–æ–±–ª–µ–º–∞ –ø—Ä–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ 
	- ![[Pasted image 20230418203319.png]]
- SmoothQuant [GitHub](https://github.com/mit-han-lab/smoothquant)
	- ![[Pasted image 20230418203458.png]]

How to make your model faster:
- Cuda
- Triton
- FastTransformers for really fast inference

## PEFT: Parameter Efficient Fine-Tuning
- [[LayerNorm Adapters]]
- [[Soft Prompt|Soft Prompt tuning]]
- [[LoRA|Low Rank Adapters]]
- [[I3]]