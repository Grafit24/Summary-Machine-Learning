#regularization
[Источник](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
P.S. нейроны - можно читать как units или выходы предыдущего слоя.
## Описание
Метод регуляризации главная идея которого заключается в отключение части нейронов с некоторой вероятностью $p$. Метод позволяет уменьшить переобучение и коадаптацию нейронов.

![[dropout_visualisation.png]]

Обычно оптимальное значение $p=0.5$.

## Подробнее
По сути, вместо одной большой сети мы обучаем $2^n$  возможных подсетей с общими весами. Во время инференса сложно брать среднее предсказания по экспоненциально большому количству подсетей, поэтому используется следующая аппроксимация. Во время инференса мы используем сеть полностью (без dropout) и домножаем веса на $1-p$ (или домножать не отключенные веса на $\frac{1}{1-p}$ во время обучения). Это гарантирует нам, что мат. ожидание выхода нейросети во время обучения совпадёт с мат. ожиданием во время инференса.
![[inference_with_dropout.png]]
Также не могу не отметить, то как применяется dropout слой в нейронной сети. Пусть dropout является $l$-ым слоем, тогда он применяется к выходу предыдущего слоя $l-1$ , что влияет на веса слоя $l+1$, но не на его bias!
![[nn_and_dropout.png]]
P.S. Мои пометки довольно условны, но надеюсь понятны.

### Forward
Мы зануляем часть входа предыдущего слоя тем самым "отключая" некоторые нейроны. (В случае, если мы не хотим во время инференса домножать на $1-p$, то домножаем не занулённые веса на $\frac{1}{1-p}$)

### Backward
Мы сохраняем маску (если домножали на $\frac{1}{1-p}$, то сохраняем это в маске), по которой зануляли вход при forward и применяем её также для градиентов при backward. 

## Code
```python
class Dropout(Module):
	def __init__(self, p=0.5):
		super().__init__()
		self.p = p
		self.mask = None

	 def forward(self, input):
		if self._train:
			p_save = 1 - self.p
			self.mask = np.random.binomial(
				1, p=p_save, size=input.shape)/p_save
			self.output = self.mask*input
		else:
			self.output = input
		return self.output

	def backward(self, input, grad_output):
		if self._train:
			grad_input = self.mask*grad_output
		else:
			grad_input = grad_output
		return grad_input
```

## Советы
**Выбор гиперпараметра $p$**, где $p$ – это вероятность того, что нейрон исчезнет из сети. Обычно оптимальное значение $p \in [0.2, 0.5]$. Для вещественного входного слоя, вроде speech frame или image patches подходит $p=0.2$. Для скрытого слоя выбор $p$ зависит от количества нейронов в слое.

**Настройска размер сети**. Пусть в каком-то скрытом слое нашей сети $n$ нейронов и они исчезают с некоторой вероятностью $p$, тогда наша сеть будь представлена $(1-p)n$ нейронами после дропаута. То есть если для некоторого слоя стандартной нейросети оптимально $n$ нейронов для решаемой задачи, то для хорошей дропаут нейросети это количество будет по крайне мере $n/(1-p)$. (Эвристика предложенная авторами статьи для full-connected и convolution сетей)  

Авторы статьи отмечают, что dropout нейросети особенно хорошо работают в купе с:
- высоким [[Momentum|momentum]]. Авторы указывают, что 0.95-0.99 работает достаточно хорошо, также можно использовать обычный [[SGD]] с learning rate в 10-100 раз больше чем обычный. Это позволяет значительно ускорить обучение.
- [[Max-norm Regularization]]. Высокий learning rate/momentum приводит к сильному увелечению значения весов. Для предотвращение этого можно использовать max-norm регулиризацию. Оптимальное значение $c$ в промежутке от 3 до 4.
- большим [[Learning Decay|learning decay]].
